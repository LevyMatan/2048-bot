<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>2048 Bot — Documentation</title>
  <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
  <style>
    :root {
      --bg: #f8fafc;
      --surface: #fff;
      --text: #1a1a2e;
      --text-muted: #64748b;
      --accent: #2563eb;
      --accent-hover: #1d4ed8;
      --accent-soft: #dbeafe;
      --border: #e2e8f0;
      --code-bg: #f1f5f9;
      --code-border: #e2e8f0;
      --nav-bg: #fff;
      --hero-gradient: linear-gradient(135deg, #1e3a5f 0%, #2563eb 100%);
      --tag-bg: #dbeafe;
      --tag-text: #1e40af;
      --table-stripe: #f8fafc;
      --shadow-sm: 0 1px 2px rgba(0,0,0,0.05);
      --shadow-md: 0 4px 6px -1px rgba(0,0,0,0.07), 0 2px 4px -2px rgba(0,0,0,0.05);
    }
    @media (prefers-color-scheme: dark) {
      :root {
        --bg: #0f172a;
        --surface: #1e293b;
        --text: #e2e8f0;
        --text-muted: #94a3b8;
        --accent: #60a5fa;
        --accent-hover: #93bbfc;
        --accent-soft: #1e3a5f;
        --border: #334155;
        --code-bg: #0f172a;
        --code-border: #334155;
        --nav-bg: #1e293b;
        --hero-gradient: linear-gradient(135deg, #0f172a 0%, #1e3a5f 100%);
        --tag-bg: #1e3a5f;
        --tag-text: #93c5fd;
        --table-stripe: #1e293b;
        --shadow-sm: 0 1px 2px rgba(0,0,0,0.2);
        --shadow-md: 0 4px 6px -1px rgba(0,0,0,0.3);
      }
    }

    * { box-sizing: border-box; margin: 0; padding: 0; }
    html { scroll-behavior: smooth; scroll-padding-top: 1.5rem; }

    body {
      font-family: 'Inter', system-ui, -apple-system, 'Segoe UI', Roboto, sans-serif;
      line-height: 1.7;
      color: var(--text);
      background: var(--bg);
    }

    /* Layout */
    .layout {
      display: grid;
      grid-template-columns: 220px 1fr;
      max-width: 1200px;
      margin: 0 auto;
      gap: 2.5rem;
      padding: 0 2rem 4rem;
    }
    @media (max-width: 860px) {
      .layout { grid-template-columns: 1fr; padding: 0 1rem 3rem; }
      nav.sidebar { position: static !important; margin-bottom: 1rem; }
    }

    /* Sidebar */
    nav.sidebar {
      position: sticky;
      top: 1rem;
      height: fit-content;
      max-height: calc(100vh - 2rem);
      overflow-y: auto;
      padding: 1rem 0;
    }
    nav.sidebar .nav-group { margin-bottom: 1rem; }
    nav.sidebar .nav-label {
      font-size: 0.7rem;
      font-weight: 700;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: var(--text-muted);
      padding: 0.25rem 0.75rem;
    }
    nav.sidebar a {
      display: block;
      color: var(--text-muted);
      text-decoration: none;
      padding: 0.3rem 0.75rem;
      font-size: 0.85rem;
      border-radius: 6px;
      transition: color 0.15s, background 0.15s;
    }
    nav.sidebar a:hover {
      color: var(--accent);
      background: var(--accent-soft);
    }
    nav.sidebar a.nav-cta {
      display: block;
      margin-top: 0.25rem;
      padding: 0.45rem 0.75rem;
      font-weight: 700;
      color: #fff;
      background: linear-gradient(135deg, #3b82f6, #8b5cf6);
      border-radius: 6px;
      text-align: center;
    }
    nav.sidebar a.nav-cta:hover {
      background: linear-gradient(135deg, #2563eb, #7c3aed);
      color: #fff;
    }

    /* Main content */
    main { min-width: 0; }

    /* Hero */
    .hero {
      background: var(--hero-gradient);
      color: #fff;
      padding: 2.5rem 2rem;
      border-radius: 12px;
      margin-bottom: 2.5rem;
      box-shadow: var(--shadow-md);
    }
    .hero h1 { font-size: 2rem; font-weight: 800; margin-bottom: 0.5rem; }
    .hero p { color: #cbd5e1; font-size: 1.05rem; margin-top: 0.5rem; }
    .hero p:last-child { margin-bottom: 0; }
    .hero a { color: #93c5fd; }
    .hero .cta-play {
      display: inline-block;
      margin-top: 0.75rem;
      padding: 0.65rem 1.6rem;
      font-size: 1.1rem;
      font-weight: 700;
      color: #fff;
      background: linear-gradient(135deg, #3b82f6, #8b5cf6);
      border-radius: 8px;
      text-decoration: none;
      box-shadow: 0 4px 14px rgba(59,130,246,0.45);
      transition: transform 0.15s, box-shadow 0.15s;
    }
    .hero .cta-play:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(59,130,246,0.55);
    }
    .hero .tags { display: flex; gap: 0.5rem; flex-wrap: wrap; margin-top: 1rem; }
    .hero .tag {
      display: inline-block;
      background: rgba(255,255,255,0.15);
      color: #e0f2fe;
      padding: 0.2rem 0.65rem;
      border-radius: 999px;
      font-size: 0.75rem;
      font-weight: 600;
    }

    /* Sections */
    section { margin-bottom: 2.5rem; }
    h2 {
      font-size: 1.4rem;
      font-weight: 700;
      margin-top: 2.5rem;
      margin-bottom: 0.75rem;
      padding-bottom: 0.4rem;
      border-bottom: 2px solid var(--border);
    }
    h3 {
      font-size: 1.1rem;
      font-weight: 600;
      margin-top: 1.5rem;
      margin-bottom: 0.5rem;
      color: var(--text);
    }
    h4 {
      font-size: 0.95rem;
      font-weight: 600;
      margin-top: 1.25rem;
      margin-bottom: 0.4rem;
    }
    p { margin: 0.6rem 0; }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }
    ul, ol { padding-left: 1.5rem; margin: 0.5rem 0; }
    li { margin: 0.25rem 0; }

    /* Code */
    pre, code {
      font-family: 'JetBrains Mono', 'Fira Code', ui-monospace, SFMono-Regular, Menlo, monospace;
      font-size: 0.82rem;
    }
    code {
      padding: 0.15em 0.4em;
      border-radius: 4px;
      background: var(--code-bg);
      border: 1px solid var(--code-border);
    }
    pre {
      background: var(--code-bg);
      padding: 1rem 1.25rem;
      border-radius: 8px;
      overflow-x: auto;
      border: 1px solid var(--code-border);
      margin: 0.75rem 0;
      line-height: 1.55;
      white-space: pre !important;
      display: block !important;
    }
    pre code {
      padding: 0;
      border: none;
      background: none;
      white-space: pre !important;
      display: block !important;
    }

    /* Tables */
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 0.75rem 0;
      font-size: 0.9rem;
    }
    th, td { padding: 0.6rem 0.85rem; text-align: left; border: 1px solid var(--border); }
    th { background: var(--code-bg); font-weight: 600; font-size: 0.82rem; }
    tr:nth-child(even) td { background: var(--table-stripe); }

    /* Cards */
    .card {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 1.25rem 1.5rem;
      margin: 0.75rem 0;
      box-shadow: var(--shadow-sm);
    }
    .card h4 { margin-top: 0; }

    /* Grid */
    .card-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 1rem;
      margin: 1rem 0;
    }

    /* Mermaid */
    .mermaid { margin: 1rem 0; }

    /* SVG board diagrams */
    .board-svg { width: 100%; max-width: 340px; height: auto; margin: 0.5rem 0; }

    /* Callout boxes */
    .callout {
      border-left: 4px solid var(--accent);
      background: var(--accent-soft);
      padding: 0.85rem 1.1rem;
      border-radius: 0 8px 8px 0;
      margin: 1rem 0;
      font-size: 0.92rem;
    }
    .callout.warning {
      border-left-color: #f59e0b;
      background: #fef3c7;
    }
    @media (prefers-color-scheme: dark) {
      .callout.warning { background: #422006; }
    }

    /* File tree */
    .file-tree {
      font-family: ui-monospace, SFMono-Regular, Menlo, monospace;
      font-size: 0.82rem;
      line-height: 1.6;
      white-space: pre;
      overflow-x: auto;
      background: var(--code-bg);
      border: 1px solid var(--code-border);
      padding: 1rem 1.25rem;
      border-radius: 8px;
      margin: 0.75rem 0;
    }
    .file-tree .dir { color: var(--accent); font-weight: 600; }
    .file-tree .comment { color: var(--text-muted); }

    /* Math-like */
    .formula {
      font-family: 'Georgia', 'Times New Roman', serif;
      font-style: italic;
      display: block;
      text-align: center;
      margin: 1rem 0;
      font-size: 1.05rem;
    }

    /* Scroll to top */
    .back-top {
      text-align: center;
      margin: 2rem 0 0;
      font-size: 0.85rem;
    }
  </style>
</head>
<body>

<div class="layout">
  <!-- Sidebar Navigation -->
  <nav class="sidebar">
    <div class="nav-group">
      <div class="nav-label">Play</div>
      <a class="nav-cta" href="play.html">&#9654; Play in Browser</a>
    </div>
    <div class="nav-group">
      <div class="nav-label">Overview</div>
      <a href="#intro">Introduction</a>
      <a href="#project-structure">Project Structure</a>
      <a href="#players">Player Types</a>
    </div>
    <div class="nav-group">
      <div class="nav-label">Getting Started</div>
      <a href="#prerequisites">Prerequisites</a>
      <a href="#building">Building</a>
      <a href="#running">Running Games</a>
      <a href="#cli-reference">CLI Reference</a>
    </div>
    <div class="nav-group">
      <div class="nav-label">TDL Deep Dive</div>
      <a href="#tdl-overview">Overview</a>
      <a href="#ntuple">N-Tuple Networks</a>
      <a href="#symmetry">Isomorphic Symmetry</a>
      <a href="#afterstate">Afterstate Value</a>
      <a href="#td-learning">TD(0) Learning</a>
      <a href="#architecture">Architecture</a>
      <a href="#training">Training the Network</a>
      <a href="#results">Results</a>
    </div>
    <div class="nav-group">
      <div class="nav-label">Browser GUI</div>
      <a href="#browser-gui">Overview</a>
      <a href="#teacher-mode">Teacher Mode</a>
      <a href="#browser-training">In-Browser Training</a>
    </div>
  </nav>

  <!-- Main Content -->
  <main>

    <!-- Hero -->
    <section class="hero" id="intro">
      <h1>2048 Bot</h1>
      <p>A high-performance 2048 AI exploring multiple strategies &mdash; from simple heuristics to a <strong>temporal-difference learning</strong> agent that regularly reaches the <strong>8192</strong> tile and beyond.</p>
      <p><a href="https://github.com/LevyMatan/2048-bot">View on GitHub</a></p>
      <a class="cta-play" href="play.html">&#9654;&ensp;Play in Browser</a>
      <div class="tags">
        <span class="tag">C++17</span>
        <span class="tag">CMake</span>
        <span class="tag">N-Tuple Networks</span>
        <span class="tag">TD Learning</span>
        <span class="tag">Expectimax</span>
        <span class="tag">Multi-threaded</span>
        <span class="tag">Browser GUI</span>
        <span class="tag">Teacher Mode</span>
      </div>
    </section>

    <!-- ==================== PROJECT OVERVIEW ==================== -->

    <section id="project-structure">
      <h2>Project Structure</h2>
      <p>The repository is organized into a C++ implementation (primary, high-performance) and a Python implementation (prototyping and visualization).</p>

      <div class="file-tree">
<span class="dir">2048-bot/</span>
<span class="dir">&#x251C;&#x2500; cpp/</span>                         <span class="comment"># C++ implementation</span>
&#x2502;   <span class="dir">&#x251C;&#x2500; src/</span>                     <span class="comment"># Source files</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; main.cpp              <span class="comment"># Entry point &amp; game runner</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; board.cpp/hpp          <span class="comment"># 64-bit board representation</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; game.cpp/hpp           <span class="comment"># Game loop &amp; mechanics</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; players.hpp            <span class="comment"># Player base class &amp; configs</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; random_player.cpp      <span class="comment"># Random move selection</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; heuristic_player.cpp   <span class="comment"># Weighted heuristic eval</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; expectimax_player.cpp  <span class="comment"># Expectimax search</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; tdl_player.cpp/hpp     <span class="comment"># TD-Learning player + multi-threaded training</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; ntuple_network.hpp     <span class="comment"># N-Tuple network impl</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; evaluation.cpp/hpp     <span class="comment"># Heuristic evaluation fns</span>
&#x2502;   &#x2502;   &#x251C;&#x2500; arg_parser.cpp/hpp     <span class="comment"># CLI argument parsing</span>
&#x2502;   &#x2502;   &#x2514;&#x2500; logger.cpp/hpp         <span class="comment"># Configurable logger</span>
&#x2502;   <span class="dir">&#x251C;&#x2500; configurations/</span>      <span class="comment"># JSON config files</span>
&#x2502;   <span class="dir">&#x251C;&#x2500; tests/</span>               <span class="comment"># Google Test suite</span>
&#x2502;   &#x2514;&#x2500; CMakeLists.txt        <span class="comment"># Build configuration</span>
<span class="dir">&#x251C;&#x2500; python/</span>                      <span class="comment"># Python implementation</span>
<span class="dir">&#x251C;&#x2500; docs/</span>                        <span class="comment"># Documentation + browser GUI</span>
&#x2502;   &#x251C;&#x2500; index.html              <span class="comment"># This documentation site</span>
&#x2502;   &#x2514;&#x2500; play.html               <span class="comment"># Browser GUI (play, train, learn)</span>
<span class="dir">&#x251C;&#x2500; .github/workflows/</span>          <span class="comment"># CI: tests, benchmarks</span>
&#x2514;&#x2500; README.md
      </div>

      <h3>Board Representation</h3>
      <p>The board is stored as a single <strong>64-bit integer</strong> (<code>BoardState = uint64_t</code>). Each of the 16 cells occupies 4 bits (one nibble), storing the log<sub>2</sub> of the tile value (0 = empty, 1 = 2, 2 = 4, ..., 15 = 32768). This compact representation enables fast bitwise operations for moves, scoring, and evaluation.</p>
      <pre><code>Nibble layout (position 0 = LSB):
Pos:  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
Row:  [--- row 0 ---] [--- row 1 ---] [--- row 2 ---] [--- row 3 ---]</code></pre>

      <h3>Build System</h3>
      <p>The C++ project uses <strong>CMake</strong> (minimum 3.10) with C++17. Key build features:</p>
      <ul>
        <li><strong>Release mode:</strong> <code>-O3 -march=native -flto -DNDEBUG</code> &mdash; maximum performance, logging compiled out</li>
        <li><strong>Debug mode:</strong> <code>-O0 -g</code> &mdash; full logging, step-through debugging with <code>--wait</code></li>
        <li><strong>Google Test:</strong> fetched automatically via <code>FetchContent</code> if not installed</li>
        <li><strong>CPack:</strong> generates distributable packages (DMG on macOS)</li>
      </ul>
    </section>

    <section id="players">
      <h2>Player Types</h2>
      <p>The bot ships with four player strategies, from baseline to state-of-the-art:</p>

      <div class="card-grid">
        <div class="card">
          <h4>Random</h4>
          <p>Picks a random legal move each turn. Baseline for comparison &mdash; average score ~1,000.</p>
          <p><code>-p random</code></p>
        </div>
        <div class="card">
          <h4>Heuristic</h4>
          <p>Scores each move using a weighted combination of features: empty cells, monotonicity, smoothness, corner value, and more.</p>
          <p><code>-p heuristic</code></p>
        </div>
        <div class="card">
          <h4>Expectimax</h4>
          <p>Depth-limited search with chance nodes for random tile placement. Uses the heuristic evaluation at leaf nodes. Adaptive depth and time limits.</p>
          <p><code>-p expectimax</code></p>
        </div>
        <div class="card">
          <h4>TDL (TD-Learning)</h4>
          <p>Learns a value function from self-play using n-tuple networks and temporal-difference updates. The strongest player &mdash; routinely reaches 8192.</p>
          <p><code>-p tdl</code></p>
        </div>
      </div>
    </section>

    <!-- ==================== GETTING STARTED ==================== -->

    <section id="prerequisites">
      <h2>Prerequisites</h2>
      <table>
        <thead><tr><th>Tool</th><th>Version</th><th>Notes</th></tr></thead>
        <tbody>
          <tr><td>C++ compiler</td><td>C++17</td><td>GCC 7+, Clang 5+, or MSVC 2017+</td></tr>
          <tr><td>CMake</td><td>&ge; 3.10</td><td></td></tr>
          <tr><td>Make / Ninja</td><td>any</td><td>Build tool (Make is default)</td></tr>
          <tr><td>Git</td><td>any</td><td>To clone the repo</td></tr>
        </tbody>
      </table>

      <p><strong>macOS:</strong> <code>xcode-select --install</code> provides everything. <strong>Ubuntu:</strong> <code>sudo apt install cmake g++</code>.</p>
    </section>

    <section id="building">
      <h2>Building from Source</h2>

      <h3>Clone and Build (Release)</h3>
      <pre><code>git clone https://github.com/LevyMatan/2048-bot.git
cd 2048-bot/cpp
mkdir -p build &amp;&amp; cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make 2048</code></pre>
      <p>The binary is created at <code>cpp/build/2048</code>.</p>

      <h3>Debug Build (for watching games)</h3>
      <pre><code>cmake .. -DCMAKE_BUILD_TYPE=Debug
make 2048</code></pre>
      <div class="callout">
        <strong>Why Debug?</strong> In Release mode, all logging is compiled out (<code>NDEBUG</code> is defined), so <code>--wait</code> and <code>--log-level debug</code> have no effect. Use a Debug build when you want to watch the bot play step-by-step.
      </div>

      <h3>Running Tests</h3>
      <pre><code>cd cpp/build
make              # builds everything including tests
ctest --output-on-failure</code></pre>
    </section>

    <section id="running">
      <h2>Running Games</h2>
      <p>All commands assume you are in <code>cpp/build/</code>.</p>

      <h3>Quick Start: Heuristic Player</h3>
      <pre><code># Play 10 games with the default heuristic player
./2048 -n 10 -p heuristic</code></pre>

      <h3>Expectimax Player</h3>
      <pre><code># Play with expectimax, depth 4, time limit 100ms
./2048 -n 5 -p expectimax --depth 4 --time 100</code></pre>

      <h3>TDL Player (Trained Network)</h3>
      <p>The TDL player requires a trained weights file. See <a href="#training">Training the Network</a> to create one, then:</p>
      <pre><code># Play 100 games using the trained TDL player, 4 threads
./2048 -n 100 -t 4 --player-config ../configurations/tdl_config.json</code></pre>
      <p>The <code>tdl_config.json</code> file tells the player where to find the weights:</p>
      <pre><code>{
  "playerType": "TDL",
  "weightsPath": "weights.bin"
}</code></pre>

      <h3>Watching a Game (Debug Build)</h3>
      <pre><code># Build in Debug mode first, then:
./2048 -n 1 -p tdl --player-config ../configurations/tdl_config.json \
       --log-level debug --wait</code></pre>
      <p>This prints the board after every move and waits for you to press Enter before continuing.</p>

      <h3>Benchmarking</h3>
      <pre><code># Run 1000 games, write stats as JSON
./2048 -n 1000 -t 4 --player-config ../configurations/tdl_config.json \
       --benchmark-output results.json</code></pre>
      <p>The output JSON includes average score, P95 score, 4K/8K hit rates, and timing.</p>
    </section>

    <section id="cli-reference">
      <h2>CLI Reference</h2>
      <table>
        <thead><tr><th>Flag</th><th>Description</th><th>Default</th></tr></thead>
        <tbody>
          <tr><td><code>-n, --games &lt;N&gt;</code></td><td>Number of games to play</td><td>1</td></tr>
          <tr><td><code>-t, --threads &lt;N&gt;</code></td><td>Number of parallel threads</td><td>1</td></tr>
          <tr><td><code>-p, --player &lt;type&gt;</code></td><td>Player: <code>random</code>, <code>heuristic</code>, <code>expectimax</code>, <code>tdl</code></td><td>heuristic</td></tr>
          <tr><td><code>-d, --depth &lt;N&gt;</code></td><td>Search depth (expectimax)</td><td>3</td></tr>
          <tr><td><code>-c, --chance &lt;N&gt;</code></td><td>Chance node coverage (expectimax)</td><td>1</td></tr>
          <tr><td><code>--time &lt;ms&gt;</code></td><td>Time limit per move</td><td>1000</td></tr>
          <tr><td><code>--adaptive</code></td><td>Enable adaptive search depth</td><td>off</td></tr>
          <tr><td><code>--player-config &lt;file&gt;</code></td><td>Load player settings from JSON</td><td>&mdash;</td></tr>
          <tr><td colspan="3" style="background: var(--code-bg); font-weight: 600;">Training</td></tr>
          <tr><td><code>--train</code></td><td>Run TD(0) training mode</td><td>&mdash;</td></tr>
          <tr><td><code>--episodes &lt;N&gt;</code></td><td>Training episodes</td><td>100000</td></tr>
          <tr><td><code>--alpha &lt;rate&gt;</code></td><td>Learning rate</td><td>0.1</td></tr>
          <tr><td><code>--weights &lt;path&gt;</code></td><td>Weights file (load/save)</td><td>weights.bin</td></tr>
          <tr><td><code>-t, --threads &lt;N&gt;</code></td><td>Parallel training threads (Hogwild)</td><td>1</td></tr>
          <tr><td colspan="3" style="background: var(--code-bg); font-weight: 600;">Logging &amp; Debug</td></tr>
          <tr><td><code>-l, --log-level &lt;lvl&gt;</code></td><td>Log level: error, warn, info, debug</td><td>error</td></tr>
          <tr><td><code>-o, --output &lt;dest&gt;</code></td><td>Output: none, console, file, both</td><td>none</td></tr>
          <tr><td><code>--wait</code></td><td>Pause between moves (press Enter)</td><td>off</td></tr>
          <tr><td><code>--benchmark-output &lt;path&gt;</code></td><td>Write benchmark stats as JSON</td><td>&mdash;</td></tr>
        </tbody>
      </table>
    </section>

    <!-- ==================== TDL DEEP DIVE ==================== -->

    <section id="tdl-overview">
      <h2>TDL Player &mdash; Deep Dive</h2>
      <p>The TDL (Temporal-Difference Learning) player is the strongest strategy in this project. It learns a <strong>value function</strong> from self-play using <strong>n-tuple networks</strong> &mdash; a pattern-based function approximator that maps board states to expected future rewards.</p>

      <p>The key ideas:</p>
      <ol>
        <li><strong>N-tuple networks</strong> provide a fast, compact way to evaluate board positions</li>
        <li><strong>Isomorphic symmetry</strong> exploits the board&rsquo;s 8-fold symmetry to generalize across equivalent positions</li>
        <li><strong>Afterstate values</strong> decouple the player&rsquo;s move from the random tile placement</li>
        <li><strong>TD(0) learning</strong> updates the value function from the difference between consecutive predictions</li>
      </ol>

      <p>This approach is based on the seminal work by Szubert &amp; Ja&#347;kowski (2014) and subsequent improvements in the literature.</p>
    </section>

    <section id="ntuple">
      <h2>N-Tuple Networks</h2>
      <p>An <strong>n-tuple network</strong> is a set of pattern-based features. Each pattern is a fixed set of board positions (e.g., six specific cells). For a given board state, we:</p>
      <ol>
        <li>Read the tile values at the pattern&rsquo;s positions (each 0&ndash;15 in log<sub>2</sub> form)</li>
        <li>Pack them into a single index: <code>index = tile[0] | (tile[1] &lt;&lt; 4) | ... | (tile[5] &lt;&lt; 20)</code></li>
        <li>Look up a learned weight in a table using that index</li>
        <li>Sum the weights across all patterns and all symmetric views</li>
      </ol>

      <h3>The Four 6-Tuple Patterns</h3>
      <p>We use four <strong>6-tuples</strong>. Each has 16<sup>6</sup> = 16,777,216 entries (one float per possible 6-tile configuration). Total: 4 patterns &times; 64 MB &asymp; <strong>256 MB</strong>.</p>
      <p>Board positions are numbered row-major:</p>
      <pre><code> 0  1  2  3
 4  5  6  7
 8  9 10 11
12 13 14 15</code></pre>

      <table>
        <thead><tr><th>Pattern</th><th>Positions</th><th>Shape</th></tr></thead>
        <tbody>
          <tr><td><strong>A</strong></td><td>{0, 1, 2, 3, 4, 5}</td><td>Top row + start of 2nd row</td></tr>
          <tr><td><strong>B</strong></td><td>{4, 5, 6, 7, 8, 9}</td><td>2nd row + start of 3rd row</td></tr>
          <tr><td><strong>C</strong></td><td>{0, 1, 2, 4, 5, 6}</td><td>L-shaped, top-left block</td></tr>
          <tr><td><strong>D</strong></td><td>{4, 5, 6, 8, 9, 10}</td><td>L-shaped, middle block</td></tr>
        </tbody>
      </table>

      <p>Pattern coverage on the board (highlighted cells):</p>
      <svg class="board-svg" viewBox="0 0 340 200" xmlns="http://www.w3.org/2000/svg">
        <!-- Pattern A: 0,1,2,3,4,5 -->
        <text x="0" y="14" font-size="12" font-weight="bold" fill="currentColor">A</text>
        <g transform="translate(0,18)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#3b82f6" opacity="0.8"/></g>
        <g transform="translate(40,18)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#3b82f6" opacity="0.8"/></g>
        <g transform="translate(80,18)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#3b82f6" opacity="0.8"/></g>
        <g transform="translate(120,18)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#3b82f6" opacity="0.8"/></g>
        <g transform="translate(0,58)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#3b82f6" opacity="0.8"/></g>
        <g transform="translate(40,58)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#3b82f6" opacity="0.8"/></g>
        <g transform="translate(80,58)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/></g>
        <g transform="translate(120,58)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/></g>
        <!-- Pattern B: 4,5,6,7,8,9 -->
        <text x="172" y="14" font-size="12" font-weight="bold" fill="currentColor">B</text>
        <g transform="translate(160,18)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/></g>
        <g transform="translate(200,18)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#22c55e" opacity="0.8"/></g>
        <g transform="translate(240,18)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#22c55e" opacity="0.8"/></g>
        <g transform="translate(280,18)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#22c55e" opacity="0.8"/></g>
        <g transform="translate(160,58)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#22c55e" opacity="0.8"/></g>
        <g transform="translate(200,58)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#22c55e" opacity="0.8"/></g>
        <g transform="translate(240,58)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#22c55e" opacity="0.8"/></g>
        <g transform="translate(280,58)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/></g>
        <!-- Pattern C: 0,1,2,4,5,6 -->
        <text x="0" y="114" font-size="12" font-weight="bold" fill="currentColor">C</text>
        <g transform="translate(0,118)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#eab308" opacity="0.8"/></g>
        <g transform="translate(40,118)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#eab308" opacity="0.8"/></g>
        <g transform="translate(80,118)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#eab308" opacity="0.8"/></g>
        <g transform="translate(120,118)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/></g>
        <g transform="translate(0,158)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#eab308" opacity="0.8"/></g>
        <g transform="translate(40,158)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#eab308" opacity="0.8"/></g>
        <g transform="translate(80,158)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#eab308" opacity="0.8"/></g>
        <g transform="translate(120,158)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/></g>
        <!-- Pattern D: 4,5,6,8,9,10 -->
        <text x="172" y="114" font-size="12" font-weight="bold" fill="currentColor">D</text>
        <g transform="translate(160,118)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/></g>
        <g transform="translate(200,118)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#a855f7" opacity="0.8"/></g>
        <g transform="translate(240,118)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#a855f7" opacity="0.8"/></g>
        <g transform="translate(280,118)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/></g>
        <g transform="translate(160,158)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#a855f7" opacity="0.8"/></g>
        <g transform="translate(200,158)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#a855f7" opacity="0.8"/></g>
        <g transform="translate(240,158)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/><rect x="2" y="2" width="32" height="32" rx="3" fill="#a855f7" opacity="0.8"/></g>
        <g transform="translate(280,158)"><rect x="0" y="0" width="36" height="36" rx="4" fill="var(--code-bg)" stroke="var(--border)"/></g>
      </svg>

      <h3>Why These Patterns?</h3>
      <p>The patterns are chosen to capture <strong>local spatial structure</strong>: row continuations (A, B) and rectangular blocks (C, D). Together they cover the most important tile arrangements for strategic play &mdash; keeping large tiles in corners, maintaining monotonic rows, and building merge chains. The 8-way symmetry (below) ensures full board coverage from just these four patterns.</p>
    </section>

    <section id="symmetry">
      <h2>Isomorphic Symmetry</h2>
      <p>A 4&times;4 board has <strong>8 symmetric forms</strong>: 4 rotations &times; 2 (identity + mirror). The same board configuration rotated or reflected should have the same value. Rather than storing separate weights for each orientation, we <strong>share weights</strong> by evaluating the board in all 8 views and summing:</p>

      <div class="mermaid">
flowchart LR
  B["Board state"]
  T1["Identity"]
  T2["Rot 90&deg;"]
  T3["Rot 180&deg;"]
  T4["Rot 270&deg;"]
  T5["Mirror"]
  T6["Mirror + 90&deg;"]
  T7["Mirror + 180&deg;"]
  T8["Mirror + 270&deg;"]
  S["&Sigma; weights"]
  B --> T1 --> S
  B --> T2 --> S
  B --> T3 --> S
  B --> T4 --> S
  B --> T5 --> S
  B --> T6 --> S
  B --> T7 --> S
  B --> T8 --> S
      </div>

      <p>Implementation uses an <strong>index board</strong> (<code>0xFEDCBA9876543210</code>) where cell <em>i</em> contains value <em>i</em>. We apply each of the 8 transforms to this index board, then read the pattern positions from the transformed result. The values we read are the <em>original cell indices</em> that map to each pattern position under that symmetry.</p>

      <p>This means for each pattern we do <strong>8 table lookups</strong> (one per symmetric view) instead of 1, but we only need <strong>one weight table</strong> per pattern. The total evaluation is:</p>
      <div class="formula">
        V(state) = &Sigma;<sub>pattern</sub> &Sigma;<sub>sym=1..8</sub> weights[index(state, pattern, sym)]
      </div>
      <p>With 4 patterns &times; 8 symmetries = <strong>32 table lookups</strong> per evaluation &mdash; extremely fast.</p>
    </section>

    <section id="afterstate">
      <h2>Afterstate Value Function</h2>
      <p>We learn the value of <strong>afterstates</strong>: the board <em>after</em> the player&rsquo;s move but <em>before</em> the random tile appears. This is more stable than valuing states after the random tile, because the same afterstate can lead to many next states depending on where the random tile lands.</p>

      <div class="mermaid">
flowchart LR
  S["State s"] -->|"player move"| A["Afterstate s'"]
  A -->|"random tile"| S2["Next state s''"]
  style A fill:#dbeafe,stroke:#2563eb,color:#1e293b
      </div>

      <h3>Move Selection</h3>
      <p>At each turn, for every legal move we compute the afterstate and evaluate:</p>
      <div class="formula">
        best move = argmax<sub>move</sub> [ reward(move) + V(afterstate(move)) ]
      </div>
      <p>The policy is <strong>greedy</strong>: pick the move with the highest immediate reward plus estimated future value. No search tree is needed &mdash; just one evaluation per legal move.</p>
    </section>

    <section id="td-learning">
      <h2>TD(0) Learning</h2>
      <p>Training is done by <strong>self-play</strong>. Each episode is one complete game. We collect a trajectory of (afterstate, reward) pairs, then perform a <strong>backward pass</strong> to update weights using the temporal-difference error.</p>

      <h3>The Update Rule</h3>
      <p>Let <em>r<sub>t</sub></em> be the reward at step <em>t</em>, <em>s'<sub>t</sub></em> the afterstate, and &alpha; the learning rate. The TD(0) update is:</p>
      <div class="formula">
        V(s'<sub>t</sub>) &larr; V(s'<sub>t</sub>) + &alpha; &middot; [ r<sub>t</sub> + V(s'<sub>t+1</sub>) &minus; V(s'<sub>t</sub>) ]
      </div>
      <p>At the terminal state, the future value is 0. We iterate <strong>backwards</strong> so that when updating step <em>t</em>, step <em>t+1</em> has already been updated.</p>

      <h3>How Weights Are Updated &mdash; Step by Step</h3>

      <p>The update rule says <code>V(s') += &alpha; &middot; error</code>, but <code>V(s')</code> isn&rsquo;t a single number in a table &mdash; it&rsquo;s the <strong>sum of 32 individual weight lookups</strong>. So how does the update actually reach the weights? Here is the exact cascade:</p>

      <div class="mermaid">
flowchart TB
  ERR["TD error for afterstate s'<br><b>delta = target &minus; V(s')</b>"]
  ADJ["Total adjustment<br><b>adj = &alpha; &middot; delta</b>"]
  NET["NTupleNetwork.update(s', adj)<br>Splits adj by 4 patterns"]
  PA["Pattern A<br>receives adj / 4"]
  PB["Pattern B<br>receives adj / 4"]
  PC["Pattern C<br>receives adj / 4"]
  PD["Pattern D<br>receives adj / 4"]
  ISO["Each pattern splits<br>its share by 8 symmetries"]
  W["Each of the 8 weight entries<br>gets <b>adj / 32</b> added"]

  ERR --> ADJ --> NET
  NET --> PA
  NET --> PB
  NET --> PC
  NET --> PD
  PA --> ISO
  PB --> ISO
  PC --> ISO
  PD --> ISO
  ISO --> W

  style ERR fill:#fee2e2,stroke:#ef4444,color:#1e293b
  style ADJ fill:#fef3c7,stroke:#f59e0b,color:#1e293b
  style NET fill:#dbeafe,stroke:#2563eb,color:#1e293b
  style W fill:#d1fae5,stroke:#22c55e,color:#1e293b
      </div>

      <h4>1. Compute the TD error</h4>
      <p>After the game ends, we walk backwards through the trajectory. For the <strong>last step</strong> (the afterstate before game over), the future value is 0:</p>
      <pre><code>target = 0                          # no future after game over
error  = target &minus; V(s'_last)        # how wrong was our estimate?</code></pre>
      <p>For <strong>earlier steps</strong>, the target incorporates the reward from that move plus the (already-updated) value of the next afterstate:</p>
      <pre><code>target = reward_t + V(s'_{t+1})     # what we should have predicted
error  = target &minus; V(s'_t)           # the surprise / prediction error</code></pre>

      <h4>2. Scale by learning rate</h4>
      <p>The raw error is scaled by the learning rate &alpha; (typically 0.1) to control how aggressively we update:</p>
      <pre><code>adj = alpha * error                 # e.g. 0.1 * error</code></pre>

      <h4>3. Distribute to the network (split by 4 patterns)</h4>
      <p>The <code>NTupleNetwork</code> holds 4 patterns. It divides the adjustment equally among them:</p>
      <pre><code>// NTupleNetwork::update(state, adj)
for each pattern p in {A, B, C, D}:
    p.update(state, adj / 4)</code></pre>

      <h4>4. Distribute within each pattern (split by 8 symmetries)</h4>
      <p>Each <code>NTuplePattern</code> evaluates the board through 8 symmetric views. It divides its share equally among those 8 lookups:</p>
      <pre><code>// NTuplePattern::update(state, adj_quarter)
for each symmetry sym in {identity, rot90, rot180, rot270,
                          mirror, mirror+90, mirror+180, mirror+270}:
    idx = compute_index(state, sym)   # pack 6 tiles into 24-bit index
    weights[idx] += adj_quarter / 8   # = original adj / 32</code></pre>

      <h4>5. The actual weight entry that changes</h4>
      <p>The index <code>idx</code> is a 24-bit integer computed by reading the 6 tile values (0&ndash;15) at the pattern&rsquo;s positions under that symmetric view and packing them:</p>
      <pre><code>idx = tile[0] | (tile[1] &lt;&lt; 4) | (tile[2] &lt;&lt; 8)
    | (tile[3] &lt;&lt; 12) | (tile[4] &lt;&lt; 16) | (tile[5] &lt;&lt; 20)</code></pre>
      <p>This index selects one entry in the pattern&rsquo;s weight table (out of 16<sup>6</sup> = 16,777,216 entries). That single float gets <code>adj / 32</code> added to it.</p>

      <div class="callout">
        <strong>Summary:</strong> Each update touches exactly <strong>32 weight entries</strong> (4 patterns &times; 8 symmetries). Each entry is nudged by <code>&alpha; &middot; error / 32</code>. Over millions of episodes, the weights converge so that <code>V(s')</code> accurately predicts the expected future score from any afterstate.
      </div>

      <h3>Concrete Example</h3>
      <p>Suppose a short game produces this 3-step trajectory (afterstate, reward):</p>
      <table>
        <thead><tr><th>Step</th><th>Afterstate</th><th>Reward</th><th>V(afterstate)</th></tr></thead>
        <tbody>
          <tr><td>0</td><td>s'<sub>0</sub></td><td>4</td><td>100.0</td></tr>
          <tr><td>1</td><td>s'<sub>1</sub></td><td>8</td><td>80.0</td></tr>
          <tr><td>2 (last)</td><td>s'<sub>2</sub></td><td>16</td><td>50.0</td></tr>
        </tbody>
      </table>
      <p>Backward pass with &alpha; = 0.1:</p>

      <div class="card">
        <h4>Step 2 (last move, no future)</h4>
        <pre><code>target = 0                          # game over, no future
error  = 0 &minus; V(s'_2) = 0 &minus; 50.0 = &minus;50.0
adj    = 0.1 &times; (&minus;50.0) = &minus;5.0

# Update: each of the 32 weight entries for s'_2 gets &minus;5.0/32 = &minus;0.15625
# After update: V(s'_2) = 50.0 + (&minus;5.0) = 45.0

target = reward_2 + V(s'_2) = 16 + 45.0 = 61.0</code></pre>
      </div>
      <div class="card">
        <h4>Step 1</h4>
        <pre><code>error  = target &minus; V(s'_1) = 61.0 &minus; 80.0 = &minus;19.0
adj    = 0.1 &times; (&minus;19.0) = &minus;1.9

# Update: each of 32 entries for s'_1 gets &minus;1.9/32 &asymp; &minus;0.059
# After update: V(s'_1) = 80.0 + (&minus;1.9) = 78.1

target = reward_1 + V(s'_1) = 8 + 78.1 = 86.1</code></pre>
      </div>
      <div class="card">
        <h4>Step 0</h4>
        <pre><code>error  = target &minus; V(s'_0) = 86.1 &minus; 100.0 = &minus;13.9
adj    = 0.1 &times; (&minus;13.9) = &minus;1.39

# Update: each of 32 entries for s'_0 gets &minus;1.39/32 &asymp; &minus;0.043
# After update: V(s'_0) = 100.0 + (&minus;1.39) = 98.61</code></pre>
      </div>

      <p>In this example, all values were adjusted <strong>downward</strong> because the game ended with less future reward than predicted. Over many games, positions that lead to high scores get pushed up and positions near game-over get pushed down &mdash; until the predictions are accurate.</p>

      <h3>Why Backward?</h3>
      <p>We iterate from the last step to the first so that when we compute <code>target = r + V(s'<sub>t+1</sub>)</code>, we use the <strong>already-updated</strong> value of <code>s'<sub>t+1</sub></code>. This creates a chain: the terminal correction (value should be 0) propagates back through the trajectory, one step at a time. Each game pushes information about the outcome back toward earlier positions.</p>

      <h3>Complete Training Episode Walkthrough</h3>

      <p>A training episode is one full game played from start to game-over. It has two phases: a <strong>forward pass</strong> (play the game, recording everything) and a <strong>backward pass</strong> (update the weights). Let&rsquo;s walk through both in detail.</p>

      <div class="mermaid">
flowchart LR
  subgraph FW ["Phase 1: Forward Pass (Play the Game)"]
    direction TB
    F1["Start: empty board + 2 random tiles"]
    F2["At each step:<br>1. Try all legal moves<br>2. Score each: R + V(afterstate)<br>3. Pick the best (greedy)<br>4. Record (afterstate, R)<br>5. Add random tile"]
    F3["Game over<br>(no legal moves)"]
    F1 --> F2 --> F3
  end
  subgraph BW ["Phase 2: Backward Pass (Update Weights)"]
    direction TB
    B1["Start from last step<br>target = 0"]
    B2["For each step (last → first):<br>error = target &minus; V(s')<br>V(s') += &alpha; &times; error<br>target = R + V(s')"]
    B3["Done: 32 weights nudged<br>per step in the game"]
    B1 --> B2 --> B3
  end
  FW --> BW

  style FW fill:#dbeafe,stroke:#2563eb,color:#1e293b
  style BW fill:#fee2e2,stroke:#ef4444,color:#1e293b
      </div>

      <h4>Phase 1: Forward Pass &mdash; How Actions Are Chosen</h4>

      <p>During training, the action is chosen <strong>exactly the same way as during play</strong>: greedily. At each step, the network tries every legal move, evaluates it, and picks the best one.</p>

      <div class="callout">
        <strong>Key point:</strong> There is no exploration (no random moves, no epsilon-greedy). The policy is always <code>argmax(R + V(afterstate))</code>. This works because the <strong>random tile placement</strong> already provides natural exploration &mdash; even with the same greedy policy, different random tiles lead to different game trajectories every time.
      </div>

      <p>Here is a concrete example. Suppose the board is:</p>
      <pre><code>  2   4   8  16
  _   2   4   8
  _   _   2   4
  _   _   _   2</code></pre>
      <p>The network tries all 4 directions:</p>

      <table>
        <thead><tr><th>Move</th><th>Afterstate (after sliding)</th><th>R (merge score)</th><th>V(afterstate)</th><th>Total = R + V</th></tr></thead>
        <tbody>
          <tr><td>&larr; Left</td><td>tiles shift left, some merge</td><td>+28</td><td>12,450</td><td><strong>12,478</strong></td></tr>
          <tr><td>&rarr; Right</td><td>tiles shift right</td><td>+0</td><td>12,890</td><td><strong>12,890</strong> &star;</td></tr>
          <tr><td>&uarr; Up</td><td>tiles shift up, some merge</td><td>+12</td><td>11,200</td><td><strong>11,212</strong></td></tr>
          <tr><td>&darr; Down</td><td>invalid (can&rsquo;t move)</td><td>&mdash;</td><td>&mdash;</td><td>&mdash;</td></tr>
        </tbody>
      </table>

      <p>The network picks <strong>&rarr; Right</strong> (total 12,890) even though it scores <strong>R = 0</strong> (no merge). Why? Because <code>V(afterstate)</code> is very high &mdash; the network has learned that the afterstate of moving right leads to better future outcomes. <strong>This is a preparation move.</strong></p>

      <p>The network records <code>(afterstate_of_right, reward=0)</code> in the path, applies the move, adds a random tile, and continues to the next step.</p>

      <h4>Phase 2: Backward Pass &mdash; How Weights Are Updated</h4>

      <p>After the game ends (no legal moves), the network walks <strong>backward</strong> through the path to update weights. Here is a 4-step game fragment showing both phases together:</p>

      <table>
        <thead><tr><th>Step</th><th>Move chosen</th><th>Reward R</th><th>V(afterstate) before update</th><th>Role</th></tr></thead>
        <tbody>
          <tr><td>0</td><td>&larr; Left</td><td>4</td><td>300.0</td><td>setup</td></tr>
          <tr style="color:#e67e22;font-weight:600"><td>1</td><td>&uarr; Up</td><td>0</td><td>280.0</td><td>preparation</td></tr>
          <tr><td>2</td><td>&rarr; Right</td><td>256</td><td>100.0</td><td>big merge</td></tr>
          <tr><td>3 (last)</td><td>&darr; Down</td><td>8</td><td>20.0</td><td>final move</td></tr>
        </tbody>
      </table>

      <p>Now the backward pass (&alpha; = 0.1):</p>

      <div class="card">
        <h4>Step 3 (last): target = 0 (game over)</h4>
        <pre><code>error  = 0 &minus; V(s'_3) = 0 &minus; 20.0 = <strong>&minus;20.0</strong>
V(s'_3) += 0.1 &times; (&minus;20.0) = &minus;2.0     →  V(s'_3) is now 18.0
   (32 weight entries each get &minus;2.0/32 = &minus;0.0625)

next target = R_3 + V(s'_3) = 8 + 18.0 = <strong>26.0</strong></code></pre>
      </div>

      <div class="card">
        <h4>Step 2: target = 26.0 (from step 3)</h4>
        <pre><code>error  = 26.0 &minus; V(s'_2) = 26.0 &minus; 100.0 = <strong>&minus;74.0</strong>
V(s'_2) += 0.1 &times; (&minus;74.0) = &minus;7.4     →  V(s'_2) is now 92.6
   (32 weight entries each get &minus;7.4/32 = &minus;0.231)

next target = R_2 + V(s'_2) = 256 + 92.6 = <strong>348.6</strong></code></pre>
        <p>The big merge reward (256) flows into the target for the next (earlier) step.</p>
      </div>

      <div class="card" style="border-left: 3px solid #e67e22;">
        <h4 style="color:#e67e22">Step 1 (preparation move!): target = 348.6</h4>
        <pre><code>error  = 348.6 &minus; V(s'_1) = 348.6 &minus; 280.0 = <strong>+68.6</strong>
V(s'_1) += 0.1 &times; 68.6 = +6.86       →  V(s'_1) is now 286.86
   (32 weight entries each get +6.86/32 = +0.214)

next target = R_1 + V(s'_1) = 0 + 286.86 = <strong>286.86</strong></code></pre>
        <p><strong>This is the key moment.</strong> Step 1 had R = 0 (no merge &mdash; it just rearranged tiles). But the target is 348.6 because steps 2 and 3 earned big rewards. The error is <strong>positive</strong>, so V goes <strong>up</strong>. The network learns: &ldquo;this board configuration is valuable because it leads to big merges later.&rdquo;</p>
      </div>

      <div class="card">
        <h4>Step 0: target = 286.86</h4>
        <pre><code>error  = 286.86 &minus; V(s'_0) = 286.86 &minus; 300.0 = <strong>&minus;13.14</strong>
V(s'_0) += 0.1 &times; (&minus;13.14) = &minus;1.314 →  V(s'_0) is now 298.69
   (32 weight entries each get &minus;1.314/32 = &minus;0.041)

(no more steps — backward pass complete)</code></pre>
      </div>

      <h4>What Changed?</h4>
      <p>In this single episode, 4 &times; 32 = <strong>128 weight entries</strong> were nudged. The net effect:</p>
      <ul>
        <li>Steps 2 and 3 (near game-over) had their values <strong>decreased</strong> &mdash; the network overestimated them</li>
        <li>Step 1 (the preparation move) had its value <strong>increased</strong> &mdash; the big merge at step 2 sent credit backward</li>
        <li>Step 0 was slightly decreased &mdash; close to correct already</li>
      </ul>
      <p>After thousands of episodes, these incremental adjustments converge: the 67 million weights encode an accurate map from &ldquo;what does the board look like?&rdquo; to &ldquo;how much score will I collect from here?&rdquo;</p>

      <h3>Training Pseudocode</h3>
      <pre><code>for episode = 1 to N:
    path = []
    state = new_game()            # two random tiles

    # ── Phase 1: Forward pass (play the game greedily) ──
    while has_legal_move(state):
        # Try every legal move, evaluate afterstate
        for each legal move m:
            afterstate = apply_move(state, m)    # slide + merge, NO random tile
            score[m]   = reward(m) + V(afterstate)

        best_move  = argmax(score)               # greedy: pick highest
        afterstate = apply_move(state, best_move)
        path.append( (afterstate, reward(best_move)) )

        state = afterstate
        state = add_random_tile(state)           # NOW add random tile

    # ── Phase 2: Backward pass (update weights) ──
    target = 0                                   # game over: no future
    for t = len(path)-1 down to 0:
        (s', r) = path[t]
        error   = target - V(s')                 # how wrong was our estimate?
        V(s')  += alpha * error                  # nudge 32 weight entries
        target  = r + V(s')                      # carry value to earlier step</code></pre>

      <div class="mermaid">
flowchart TB
  subgraph episode ["One Training Episode"]
    direction TB
    Init["Init board<br>(2 random tiles)"]
    Eval["Try all legal moves<br>Score each: R + V(afterstate)"]
    Pick["Pick best move (greedy)<br>Record (afterstate, R) in path"]
    Rand["Add random tile to board"]
    Check{"Legal moves<br>available?"}
    Back["Backward pass:<br>TD(0) weight updates<br>(last step → first step)"]
    Init --> Eval --> Pick --> Rand --> Check
    Check -->|yes| Eval
    Check -->|no| Back
  end
      </div>
    </section>

    <section id="architecture">
      <h2>Architecture</h2>
      <p>The implementation consists of three main components:</p>

      <div class="mermaid">
flowchart TB
  subgraph network ["NTupleNetwork"]
    P1["NTuplePattern A<br>weights: 16^6 floats"]
    P2["NTuplePattern B<br>weights: 16^6 floats"]
    P3["NTuplePattern C<br>weights: 16^6 floats"]
    P4["NTuplePattern D<br>weights: 16^6 floats"]
  end
  TDL["TDLPlayer"]
  TDL --> network
  TDL -->|"chooseAction(state)"| eval["For each legal move:<br>reward + estimate(afterstate)"]
  eval --> network
  eval --> best["Return best move"]

  style TDL fill:#dbeafe,stroke:#2563eb,color:#1e293b
  style network fill:#f0fdf4,stroke:#22c55e,color:#1e293b
      </div>

      <h3><code>NTuplePattern</code></h3>
      <ul>
        <li>Holds one weight table of size 16<sup>6</sup> (16,777,216 floats)</li>
        <li>Precomputes the 8 isomorphic index mappings at construction time</li>
        <li><code>estimate(state)</code> &rarr; sum of 8 table lookups</li>
        <li><code>update(state, adjust)</code> &rarr; add <code>adjust/8</code> to each of the 8 entries</li>
      </ul>

      <h3><code>NTupleNetwork</code></h3>
      <ul>
        <li>Holds 4 <code>NTuplePattern</code> instances</li>
        <li><code>estimate(state)</code> &rarr; sum across all 4 patterns (32 lookups total)</li>
        <li><code>update(state, adjust)</code> &rarr; distributes <code>adjust/4</code> to each pattern</li>
        <li><code>save(path)</code> / <code>load(path)</code> for binary serialization of weights</li>
      </ul>

      <h3><code>TDLPlayer</code></h3>
      <ul>
        <li>Holds a <code>shared_ptr&lt;NTupleNetwork&gt;</code></li>
        <li><code>chooseAction(state)</code>: enumerates legal moves, evaluates each as <code>score + network.estimate(afterstate)</code>, returns the best</li>
        <li><code>trainNetwork()</code>: static method that runs self-play episodes and updates the network with TD(0). Supports <strong>multi-threaded Hogwild training</strong> for near-linear speedup across cores.</li>
        <li>Supports loading pre-trained weights for instant play</li>
      </ul>
    </section>

    <section id="training">
      <h2>Training the Network</h2>

      <h3>Basic Training</h3>
      <pre><code># From cpp/build/ (Release build recommended for speed)
./2048 --train --episodes 100000 --alpha 0.1 --weights weights.bin</code></pre>

      <h3>Multi-Threaded Training</h3>
      <p>Use <code>-t</code> to parallelize training across CPU cores using <strong>Hogwild-style</strong> asynchronous updates:</p>
      <pre><code># Train with 10 threads (e.g. Apple M4)
./2048 --train --episodes 100000 --alpha 0.1 --weights weights.bin -t 10

# Continue training with lower learning rate
./2048 --train --episodes 100000 --alpha 0.01 --weights weights.bin -t 10</code></pre>

      <p>Each thread plays its own games and updates the shared weight tables without locks. This works because individual float updates are tiny (<code>&alpha; &times; error / 32</code>) and the stochastic nature of TD learning tolerates minor data races. In practice, Hogwild converges as well as sequential training and scales nearly linearly with core count.</p>

      <h3>Training Options</h3>
      <table>
        <thead><tr><th>Flag</th><th>Description</th></tr></thead>
        <tbody>
          <tr><td><code>--episodes &lt;N&gt;</code></td><td>Number of self-play games. More = stronger. 100K is a good start; 1M for maximum strength.</td></tr>
          <tr><td><code>--alpha &lt;rate&gt;</code></td><td>Learning rate. 0.1 works well. Lower values (0.01) for fine-tuning after initial training.</td></tr>
          <tr><td><code>--weights &lt;path&gt;</code></td><td>Path to weights file. If it exists, training <strong>resumes</strong> from it. Saved at the end.</td></tr>
          <tr><td><code>-t, --threads &lt;N&gt;</code></td><td>Number of parallel training threads. Default: 1. Recommended: number of CPU cores.</td></tr>
        </tbody>
      </table>

      <div class="callout">
        <strong>Resume training:</strong> Running the same command again will load the existing <code>weights.bin</code> and continue training. This lets you incrementally improve the network without restarting from scratch. Decrease <code>--alpha</code> in later rounds for fine-tuning.
      </div>

      <h3>Training Output</h3>
      <p>Stats are printed every 1,000 episodes (or every 100 for short runs):</p>
      <pre><code>1000    avg = 4523      max = 18244
        2       100%    (0%)
        4       100%    (0%)
        ...
        2048    42.3%   (25.1%)
        4096    17.2%   (17.2%)
...
100000  avg = 78432     max = 289012
        2048    100%    (3.2%)
        4096    96.8%   (18.5%)
        8192    78.3%   (78.3%)</code></pre>
      <p>Each line shows: tile value, cumulative reach rate, and (terminal rate &mdash; fraction of games where that was the highest tile).</p>

      <h3>Training Progression</h3>
      <div class="mermaid">
xychart-beta
  title "Approximate Average Score vs Training Episodes"
  x-axis ["1K", "10K", "50K", "100K", "500K", "1M"]
  y-axis "Average Score" 0 --> 160000
  bar [4000, 30000, 65000, 80000, 120000, 140000]
      </div>
    </section>

    <section id="results">
      <h2>Results</h2>
      <p>Approximate performance after training (single run, 4&times;6-tuple, &alpha;=0.1):</p>

      <table>
        <thead>
          <tr><th>Episodes</th><th>Avg Score</th><th>Reach 2048</th><th>Reach 4096</th><th>Reach 8192</th></tr>
        </thead>
        <tbody>
          <tr><td>1K</td><td>~4,000</td><td>~40%</td><td>~10%</td><td>~0%</td></tr>
          <tr><td>10K</td><td>~30,000</td><td>~100%</td><td>~80%</td><td>~5&ndash;15%</td></tr>
          <tr><td>100K</td><td>~80,000</td><td>~100%</td><td>~95%</td><td>~15&ndash;25%</td></tr>
          <tr><td>1M</td><td>~135,000</td><td>~100%</td><td>~93%</td><td>~70&ndash;75%</td></tr>
        </tbody>
      </table>

      <p>After 1M episodes, the TDL player reaches the <strong>8192 tile in about 70&ndash;75% of games</strong> and occasionally achieves the <strong>16384 tile</strong> (~3&ndash;4% of games). Training 1M episodes takes approximately 15&ndash;30 minutes single-threaded, or <strong>2&ndash;5 minutes with multi-threaded Hogwild training</strong> (e.g., <code>-t 10</code> on an Apple M4).</p>

      <h3>Comparison Across Players</h3>
      <table>
        <thead>
          <tr><th>Player</th><th>Avg Score</th><th>4096 Rate</th><th>8192 Rate</th><th>Speed (games/sec)</th></tr>
        </thead>
        <tbody>
          <tr><td>Random</td><td>~1,000</td><td>&lt;1%</td><td>0%</td><td>very fast</td></tr>
          <tr><td>Heuristic</td><td>~8,000</td><td>~15%</td><td>0%</td><td>fast</td></tr>
          <tr><td>Expectimax (d=4)</td><td>~20,000</td><td>~60%</td><td>&lt;1%</td><td>~5&ndash;20</td></tr>
          <tr><td>TDL (1M trained)</td><td>~135,000</td><td>~93%</td><td>~70&ndash;75%</td><td>~100&ndash;300</td></tr>
        </tbody>
      </table>

      <div class="callout">
        The TDL player is not only <strong>much stronger</strong> than other players but also <strong>faster</strong> at inference time &mdash; each move requires only 32 table lookups (no tree search), making it highly suitable for real-time play and large-scale benchmarking.
      </div>
    </section>

    <!-- ==================== BROWSER GUI ==================== -->

    <section id="browser-gui">
      <h2>Browser GUI</h2>
      <p>The project includes a full-featured <strong><a href="play.html">browser-based GUI</a></strong> (<code>docs/play.html</code>) that runs entirely client-side &mdash; no server needed. Open it directly in any modern browser.</p>

      <div class="card-grid">
        <div class="card">
          <h4>Manual Play</h4>
          <p>Play 2048 with keyboard (arrow keys) or touch/swipe controls. Includes undo, score tracking, and tile animations.</p>
        </div>
        <div class="card">
          <h4>AI Players</h4>
          <p>Watch any of the four AI strategies play: Random, Heuristic, Expectimax, and TDL. Adjustable speed from slow step-by-step to max speed.</p>
        </div>
        <div class="card">
          <h4>TDL Weight Loading</h4>
          <p>Load your trained <code>weights.bin</code> file directly in the browser. The JavaScript implementation mirrors the C++ n-tuple network exactly, including the binary file format.</p>
        </div>
        <div class="card">
          <h4>Weight Export</h4>
          <p>Save trained weights back to a <code>weights.bin</code> file &mdash; compatible with the C++ program. Train in the browser, play from the CLI, or vice versa.</p>
        </div>
      </div>
    </section>

    <section id="teacher-mode">
      <h2>Teacher Mode</h2>
      <p>Teacher Mode provides a <strong>visual, step-by-step explanation</strong> of how each AI player computes its next move. Enable it by clicking the <strong>Teacher</strong> button while playing.</p>

      <h3>For the TDL Player</h3>
      <p>Teacher Mode shows the complete evaluation pipeline:</p>
      <ol>
        <li><strong>Afterstate simulation</strong> &mdash; each possible move produces a different afterstate board</li>
        <li><strong>Pattern lookup visualization</strong> &mdash; see exactly which 6 board positions each pattern reads, what tile values are there, and how they pack into a 24-bit index</li>
        <li><strong>Shared table insight</strong> &mdash; all 8 symmetries (rotations + mirrors) index into the <em>same</em> weight table per pattern (4 tables total, not 32)</li>
        <li><strong>Per-symmetry breakdown</strong> &mdash; a table showing all 8 lookups for each pattern: positions, tile values, index, and weight</li>
        <li><strong>Final decision</strong> &mdash; <code>R(s,a) + V(s&prime;)</code> for each move, with the best highlighted</li>
      </ol>

      <h3>TD(0) Training Explained</h3>
      <p>Teacher Mode also includes a deep-dive into <strong>how weights are updated</strong> during training:</p>
      <ul>
        <li><strong>Forward pass</strong> &mdash; the game path of afterstates and rewards</li>
        <li><strong>Backward pass</strong> &mdash; step-by-step TD(0) update with <code>target</code>, <code>error</code>, <code>old V</code>, and <code>new V</code></li>
        <li><strong>Preparation moves</strong> &mdash; highlighted moves where <code>R = 0</code> (no merge) but value <em>increased</em>, demonstrating how TD learning assigns credit to strategic positioning</li>
        <li><strong>Weight cascade</strong> &mdash; how the error is distributed across 4 patterns &times; 8 symmetries = 32 weight entries</li>
      </ul>

      <h3>For Other Players</h3>
      <ul>
        <li><strong>Heuristic</strong> &mdash; component breakdown (empty tiles, monotonicity, smoothness, corner value) with weighted bar chart</li>
        <li><strong>Expectimax</strong> &mdash; expected value and computation time per move</li>
        <li><strong>Random</strong> &mdash; probability distribution over legal moves</li>
      </ul>
    </section>

    <section id="browser-training">
      <h2>In-Browser Training</h2>
      <p>The browser GUI includes a <strong>live training engine</strong> that runs TD(0) self-play directly in JavaScript. Select the TDL player, enable Teacher Mode, and scroll to the <strong>Train the Network</strong> section.</p>

      <h3>Features</h3>
      <ul>
        <li><strong>Train from scratch</strong> or <strong>continue from loaded weights</strong></li>
        <li>Configurable episodes and learning rate (&alpha;)</li>
        <li><strong>Live statistics</strong> &mdash; average score, best score, max tile, tile distribution</li>
        <li><strong>Backward pass replay</strong> &mdash; after each training game, see the full TD(0) backward pass with step-by-step detail</li>
        <li><strong>Preparation move detection</strong> &mdash; automatically identifies and highlights moves that scored 0 points but gained value through the backward pass</li>
        <li><strong>Save trained weights</strong> &mdash; download the updated <code>weights.bin</code> file to use with the C++ program</li>
      </ul>

      <div class="callout">
        <strong>Note:</strong> Browser training is great for learning and experimentation, but for serious training (100K+ episodes), use the C++ implementation with multi-threaded training for orders-of-magnitude better performance.
      </div>

      <h3>Workflow</h3>
      <pre><code>1. Open the Play in Browser page (or docs/play.html locally)
2. TDL player and Teacher Mode are enabled by default
3. (Optional) Load existing weights.bin
4. Scroll to "Train the Network" section
5. Set episodes and alpha, click "Train"
6. Watch live stats and the backward pass visualization
7. Click "Save weights.bin" to export</code></pre>
    </section>

    <div class="back-top">
      <a href="#intro">&uarr; Back to top</a>
    </div>

  </main>
</div>

<script>
  mermaid.initialize({
    startOnLoad: true,
    theme: 'base',
    themeVariables: {
      primaryColor: '#dbeafe',
      primaryTextColor: '#1e293b',
      primaryBorderColor: '#93c5fd',
      lineColor: '#64748b',
      secondaryColor: '#e0f2fe',
      tertiaryColor: '#f8fafc'
    }
  });

  /* Fix for browsers that collapse whitespace inside <pre> elements */
  document.querySelectorAll('pre').forEach(function(pre) {
    var code = pre.querySelector('code');
    var el = code || pre;
    var text = el.textContent;
    if (text && text.includes('\n')) {
      var lines = text.split('\n');
      el.innerHTML = lines.map(function(l) {
        return l.replace(/ /g, '&nbsp;');
      }).join('<br>');
    }
  });
</script>
</body>
</html>
